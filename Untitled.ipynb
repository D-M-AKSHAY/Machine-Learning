{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ae0e868-2122-40a5-9355-f25fce86521a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept (Beta_0): 1.2363636363636368\n",
      "Slope (Beta_1): 1.1696969696969697\n",
      "SSE: 5.624242424242421\n",
      "R^2: 0.952538038613988\n"
     ]
    }
   ],
   "source": [
    "#for the 1st question\n",
    "import numpy as np\n",
    "# Input data\n",
    "x_values = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "y_values = np.array([1, 3, 2, 5, 7, 8, 8, 9, 10, 12])\n",
    "# Number of data points\n",
    "n = len(x_values)\n",
    "# Calculate coefficients\n",
    "slope = (n * np.sum(x_values * y_values) - np.sum(x_values) * np.sum(y_values)) / (n * np.sum(x_values**2) - (np.sum(x_values))**2)\n",
    "intercept = (np.sum(y_values) - slope * np.sum(x_values)) / n\n",
    "# Predicted y values\n",
    "y_predicted = intercept + slope * x_values\n",
    "# Calculate Sum of Squared Errors (SSE)\n",
    "sse = np.sum((y_values - y_predicted)**2)\n",
    "# Mean of y values\n",
    "mean_y_values = np.mean(y_values)\n",
    "# Calculate Total Sum of Squares (SS Total)\n",
    "ss_total = np.sum((y_values - mean_y_values)**2)\n",
    "# Calculate R-squared\n",
    "r_squared = 1 - (sse / ss_total)\n",
    "# Print results\n",
    "print(\"Intercept (Beta_0):\", intercept)\n",
    "print(\"Slope (Beta_1):\", slope)\n",
    "print(\"SSE:\", sse)\n",
    "print(\"R^2:\", r_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d8ffd0a-6572-4367-9a39-9d2f79ab0071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytic solution:\n",
      "SSE: 5.624242424242423\n",
      "R squared: 0.952538038613988\n",
      "\n",
      "Full-batch Gradient Descent:\n",
      "SSE: 5.624278989977716\n",
      "R squared: 0.9525377300423822\n",
      "\n",
      "Stochastic Gradient Descent:\n",
      "SSE: 7.575559791810393\n",
      "R squared: 0.9360712253855663\n"
     ]
    }
   ],
   "source": [
    "#1st and 2nd question are executed in the same block\n",
    "import numpy as np\n",
    "\n",
    "x_vals = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "y_vals = np.array([1, 3, 2, 5, 7, 8, 8, 9, 10, 12])\n",
    "\n",
    "def linear_regression_analytic(x, y):\n",
    "    n = len(x)\n",
    "    x_mean = np.mean(x)\n",
    "    y_mean = np.mean(y)\n",
    "    \n",
    "    numerator = np.sum((x - x_mean) * (y - y_mean))\n",
    "    denominator = np.sum((x - x_mean) ** 2)\n",
    "    slope = numerator / denominator\n",
    "    intercept = y_mean - slope * x_mean\n",
    "    \n",
    "    y_pred = slope * x + intercept\n",
    "    sse = np.sum((y - y_pred) ** 2)\n",
    "    \n",
    "    ss_total = np.sum((y - y_mean) ** 2)\n",
    "    r_squared = 1 - (sse / ss_total)\n",
    "    \n",
    "    return sse, r_squared\n",
    "\n",
    "def linear_regression_gradient_descent(x, y, lr, num_iters):\n",
    "    n = len(x)\n",
    "    slope = 0\n",
    "    intercept = 0\n",
    "    \n",
    "    for _ in range(num_iters):\n",
    "        y_pred = slope * x + intercept\n",
    "        d_slope = (-2/n) * np.sum(x * (y - y_pred))\n",
    "        d_intercept = (-2/n) * np.sum(y - y_pred)\n",
    "        \n",
    "        slope -= lr * d_slope\n",
    "        intercept -= lr * d_intercept\n",
    "    \n",
    "    y_pred_final = slope * x + intercept\n",
    "    sse = np.sum((y - y_pred_final) ** 2)\n",
    "    \n",
    "    y_mean = np.mean(y)\n",
    "    ss_total = np.sum((y - y_mean) ** 2)\n",
    "    r_squared = 1 - (sse / ss_total)\n",
    "    \n",
    "    return sse, r_squared\n",
    "\n",
    "def linear_regression_stochastic_gradient_descent(x, y, lr, num_iters):\n",
    "    n = len(x)\n",
    "    slope = 0\n",
    "    intercept = 0\n",
    "    \n",
    "    for _ in range(num_iters):\n",
    "        for i in range(n):\n",
    "            y_pred = slope * x[i] + intercept\n",
    "            d_slope = (-2) * x[i] * (y[i] - y_pred)\n",
    "            d_intercept = (-2) * (y[i] - y_pred)\n",
    "            \n",
    "            slope -= lr * d_slope\n",
    "            intercept -= lr * d_intercept\n",
    "    \n",
    "    y_pred_final = slope * x + intercept\n",
    "    sse = np.sum((y - y_pred_final) ** 2)\n",
    "    \n",
    "    y_mean = np.mean(y)\n",
    "    ss_total = np.sum((y - y_mean) ** 2)\n",
    "    r_squared = 1 - (sse / ss_total)\n",
    "    \n",
    "    return sse, r_squared\n",
    "\n",
    "lr = 0.01\n",
    "num_iters = 1000\n",
    "lr_stochastic = 0.01\n",
    "num_iters_stochastic = 100\n",
    "\n",
    "sse_analytic, r_squared_analytic = linear_regression_analytic(x_vals, y_vals)\n",
    "sse_gd_full, r_squared_gd_full = linear_regression_gradient_descent(x_vals, y_vals, lr, num_iters)\n",
    "sse_gd_stochastic, r_squared_gd_stochastic = linear_regression_stochastic_gradient_descent(x_vals, y_vals, lr_stochastic, num_iters_stochastic)\n",
    "\n",
    "print(\"Analytic solution:\")\n",
    "print(\"SSE:\", sse_analytic)\n",
    "print(\"R squared:\", r_squared_analytic)\n",
    "\n",
    "print(\"\\nFull-batch Gradient Descent:\")\n",
    "print(\"SSE:\", sse_gd_full)\n",
    "print(\"R squared:\", r_squared_gd_full)\n",
    "\n",
    "print(\"\\nStochastic Gradient Descent:\")\n",
    "print(\"SSE:\", sse_gd_stochastic)\n",
    "print(\"R squared:\", r_squared_gd_stochastic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d815b32-5820-44ff-bcc4-0841938fc821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best attribute based on correlation with 'MEDV': LSTAT\n",
      "Analytic formulation coefficients: [34.23579926 -0.93006897]\n",
      "Gradient Descent (Full-batch) coefficients: [1.09075655]\n",
      "Stochastic Gradient Descent coefficients: [0.89061244]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('HousingData.csv', skiprows=1, header=None, names=['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV'], na_values='NA')\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "def linear_regression_analytic(X, y):\n",
    "    X = np.column_stack((np.ones(len(X)), X))  # Add a column of ones for intercept\n",
    "    coefficients = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "    return coefficients\n",
    "\n",
    "def linear_regression_gradient_descent(X, y, learning_rate, num_iterations):\n",
    "    n_samples, n_features = X.shape\n",
    "    coefficients = np.zeros(n_features)\n",
    "    \n",
    "    for _ in range(num_iterations):\n",
    "        error = np.dot(X, coefficients) - y\n",
    "        gradient = np.dot(X.T, error) / n_samples\n",
    "        coefficients -= learning_rate * gradient\n",
    "    \n",
    "    return coefficients\n",
    "\n",
    "def linear_regression_stochastic_gradient_descent(X, y, learning_rate, num_iterations):\n",
    "    n_samples, n_features = X.shape\n",
    "    coefficients = np.zeros(n_features)\n",
    "    \n",
    "    for _ in range(num_iterations):\n",
    "        for i in range(n_samples):\n",
    "            error = np.dot(X[i], coefficients) - y[i]\n",
    "            gradient = X[i] * error\n",
    "            coefficients -= learning_rate * gradient\n",
    "    \n",
    "    return coefficients\n",
    "\n",
    "correlation = df.corr()['MEDV'].abs().sort_values(ascending=False)\n",
    "best_attribute = correlation.index[1]  \n",
    "\n",
    "X = df[best_attribute].values.reshape(-1, 1)\n",
    "y = df['MEDV'].values\n",
    "\n",
    "learning_rate = 0.0001\n",
    "num_iterations = 1000\n",
    "\n",
    "coefficients_analytic = linear_regression_analytic(X, y)\n",
    "coefficients_gd = linear_regression_gradient_descent(X, y, learning_rate, num_iterations)\n",
    "coefficients_sgd = linear_regression_stochastic_gradient_descent(X, y, learning_rate, num_iterations)\n",
    "\n",
    "print(\"Best attribute based on correlation with 'MEDV':\", best_attribute)\n",
    "print(\"Analytic formulation coefficients:\", coefficients_analytic)\n",
    "print(\"Gradient Descent (Full-batch) coefficients:\", coefficients_gd)\n",
    "print(\"Stochastic Gradient Descent coefficients:\", coefficients_sgd)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
